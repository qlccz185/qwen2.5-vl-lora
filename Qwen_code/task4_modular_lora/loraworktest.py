# -*- coding: utf-8 -*-
import torch
import numpy as np
from transformers import Qwen2_5_VLForConditionalGeneration

# ============================================================
# é…ç½®åŒºï¼ˆè¯·ä¿®æ”¹æˆä½ è‡ªå·±çš„è·¯å¾„ï¼‰
# ============================================================
ckpt_path = "/root/autodl-tmp/task4_modular_lora/training/frozen_head_vit_lora.pt"
model_path = "/root/autodl-tmp/Qwen2.5-VL-7B-Instruct"
device = "cuda" if torch.cuda.is_available() else "cpu"

# ============================================================
# 1ï¸âƒ£ åŠ è½½åŸºç¡€æ¨¡å‹å’Œ LoRA æƒé‡
# ============================================================
print("ğŸš€ Loading Qwen2.5-VL base model...")
qwen = Qwen2_5_VLForConditionalGeneration.from_pretrained(
    model_path,
    device_map="auto",
    torch_dtype=torch.bfloat16 if device == "cuda" else torch.float32,
)
visual_module = qwen.visual

print("ğŸ“‚ Loading LoRA checkpoint:", ckpt_path)
ckpt = torch.load(ckpt_path, map_location="cpu")
visual_lora = ckpt.get("visual_lora", None)

if visual_lora is None:
    raise ValueError("âŒ visual_lora not found in checkpoint keys! Available:", ckpt.keys())
print(f"âœ… visual_lora loaded with {len(visual_lora)} tensors")

# ============================================================
# 2ï¸âƒ£ æ£€æŸ¥ LoRA å‚æ•°åä¸æ¨¡å‹ç»“æ„æ˜¯å¦å¯¹é½
# ============================================================
model_keys = [n for n, _ in visual_module.named_parameters()]
lora_keys = [k for k in visual_lora.keys() if "lora" in k]

print("\nğŸ” Checking alignment between LoRA and model...")
aligned, missing = [], []
for k in lora_keys:
    if any(k.endswith(mk) or k == mk for mk in model_keys):
        aligned.append(k)
    else:
        missing.append(k)

print(f"âœ… Aligned LoRA params: {len(aligned)}")
print(f"âš ï¸ Unmatched params: {len(missing)}")
if missing:
    print("âš ï¸ Example unmatched keys:")
    for k in missing[:10]:
        print("   ", k)

# ============================================================
# 3ï¸âƒ£ ç»Ÿè®¡ LoRA å‚æ•°çš„å‡å€¼ / æ–¹å·®
# ============================================================
print("\nğŸ“Š LoRA weight statistics:")
means, stds = [], []
for name, w in visual_lora.items():
    if "lora" in name:
        m, s = w.mean().item(), w.std().item()
        means.append(abs(m))
        stds.append(s)
        if np.random.rand() < 0.02:  # éšæœºå±•ç¤ºå°‘é‡å±‚
            print(f"  {name:<70} mean={m:.5f} std={s:.5f}")

print(f"\nğŸ“ˆ Average |mean| across LoRA weights: {np.mean(means):.5f}")
print(f"ğŸ“ˆ Average std across LoRA weights:     {np.mean(stds):.5f}")

# ============================================================
# 4ï¸âƒ£ ä¸åŸæ¨¡å‹å‚æ•°å¯¹æ¯”ï¼šæ£€æµ‹æ˜¯å¦æ›´æ–°
# ============================================================
print("\nğŸ”¬ Comparing LoRA weights to base model parameters...")
diffs = []
for name, lora_tensor in visual_lora.items():
    if name in visual_module.state_dict():
        base_tensor = visual_module.state_dict()[name].cpu().to(dtype=lora_tensor.dtype)
        delta = torch.norm(lora_tensor - base_tensor).item()
        diffs.append(delta)

if len(diffs) == 0:
    print("âš ï¸ No directly matched parameters for numeric comparison.")
else:
    mean_diff = np.mean(diffs)
    print(f"ğŸ“Š Mean absolute parameter diff vs base: {mean_diff:.6f}")
    if mean_diff < 1e-6:
        print("âŒ LoRA appears identical to base model (not trained or not injected)")
    elif mean_diff < 1e-3:
        print("âš ï¸ Very small change â€” training effect might be weak")
    else:
        print("âœ… Significant parameter change detected â€” LoRA updated successfully")

# ============================================================
# 5ï¸âƒ£ æ€»ç»“æŠ¥å‘Š
# ============================================================
total_params = sum(p.numel() for p in visual_lora.values())
print("\n================== SUMMARY ==================")
print(f"ğŸ“¦ Total LoRA tensors: {len(visual_lora)}")
print(f"ğŸ§© Trainable LoRA tensors: {len(lora_keys)}")
print(f"ğŸ“Š Total parameters (LoRA state_dict): {total_params / 1e6:.2f} M")
print("=============================================")

