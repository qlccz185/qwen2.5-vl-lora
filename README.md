# Qwen2.5-VL LoRA Pipelines

<p align="center">
  <img src="modelimages/logo.png" alt="logo" width="30%" />
</p>

This repository organizes a complete **multi-stage LoRA training, evaluation, and inference pipeline** built around **Qwen2.5-VL-7B-Instruct**.  
It covers visual-backbone LoRA insertion, intermediate-feature-based forensic head training, heatmap generation, and multimodal reasoning that integrates visual evidence into textual outputs.

![Data and Output Overview](image.png)

---

## ğŸ“‚ Directory Overview

| Directory | Description |
| --- | --- |
| `Qwen_pretrain/` | Scripts and outputs from the pre-training phase, including visual LoRA adapters (`lora_adapter/`) and forensic head weights (`head/`) |
| `Qwen_pretrain/script/` | Three-stage training scripts: classification-head warm-up, joint classification + heatmap training, and LoRA-based joint fine-tuning |
| `Qwen_code/task7_vit_lora_eval/` | Task 7 â€“ Evaluation: load visual LoRA and frozen forensic head to output classification & heatmap metrics |
| `Qwen_code/task8_inference/` | Task 8 â€“ Inference: generate explanatory text and visual evidence based on LoRA + forensic head |
| `data/` | Dataset splits in JSON format (e.g., `trainingset2/train_idx.json`, `val_idx.json`) |

---

## âš™ï¸ Environment Setup

1. Recommended Python â‰¥ 3.10 and PyTorch â‰¥ 2.1 with CUDA 12.  
2. Install dependencies (adjust `torch` version according to your CUDA runtime):

   ```bash
   pip install transformers==4.39.3 peft==0.10.0 accelerate==0.29.3
   pip install pillow numpy pandas scikit-learn tqdm
   ```

3. Organize model and data under a fixed root path (default: `/root/autodl-tmp`):

   ```text
   /root/autodl-tmp/
   â”œâ”€â”€ Qwen2.5-VL-7B-Instruct/        # Official base model
   â”œâ”€â”€ qwen2.5-vl-lora/               # This repository (recommended to git clone here)
   â”œâ”€â”€ data/
   â”‚   â”œâ”€â”€ trainingset2/
   â”‚   â”‚   â”œâ”€â”€ train_idx.json
   â”‚   â”‚   â””â”€â”€ val_idx.json
   â”‚   â””â”€â”€ testset2/annotation.json   # For Task 8 inference, can be replaced
   â””â”€â”€ task*_*/                       # Output directories generated by scripts
   ```

   If resources are stored elsewhere, override paths via CLI arguments or config files.

---

## ğŸ§¾ Annotation File Format

Training and evaluation scripts read JSON lists where each entry contains the image path, optional mask, and label. Example:

```json
[
  {
    "image_path": "trainingset2/image/xxx.jpg",
    "mask_path": "trainingset2/spatial_localize/xxx.jpg",
    "label": 1
  },
  {
    "image_path": "trainingset2/image/yyy.jpg",
    "mask_path": "1080x1920",
    "label": 0
  }
]
```

- `label = 1` â†’ fake image (a corresponding mask can be provided);  
- Real images may use `HxW` string for `mask_path` to specify original size;  
- All paths are concatenated with `data_root` (default `/root/autodl-tmp/data`).

---

## ğŸ§  Training Pipeline

### ğŸš€ Stage A â€“ Classification-Head Warm-Up  
`Qwen_pretrain/script/warmclassification.py`

- Freeze the Qwen2.5-VL visual and language towers; train only the fusion neck and binary classification head.  
- Reads data from `/root/data/trainingset2/trainingset2/annotation.json` (modifiable).  
- Key arguments: `--epochs`, `--batch_size`, `--lr`, `--val_ratio`.  
- Output directory (default `outputs_clsA/`) saves `best_val_auc.pt` and training logs.  
- The best classification head is used as initialization for the next stages.

**Example:**
```bash
python Qwen_pretrain/script/warmclassification.py   --epochs 4 --batch_size 32 --out_dir /root/autodl-tmp/taskA_cls
```

---

### ğŸ”¥ Stage B â€“ Joint Classification + Heatmap Training  
`Qwen_pretrain/script/classanddetect.py`

- Freeze the visual backbone; jointly optimize classification and heatmap branches.  
- Requires outputs from Stage A (`best_by_AUROC.pt`, `calibration.json`).  
- Specify data via `--data_root`, `--train_ann`, `--val_ann`.  
- Evaluation logs include classification metrics (AUROC/ACC/F1) and mask metrics (IoU/Dice).  
- Output directory (default `outputs_joint/`) contains `best_by_AUROC_joint.pt` and heatmap snapshots.

**Example:**
```bash
python Qwen_pretrain/script/classanddetect.py   --data_root /root/autodl-tmp/data   --train_ann /root/autodl-tmp/data/trainingset2/train_idx.json   --val_ann /root/autodl-tmp/data/trainingset2/val_idx.json   --clsA_dir /root/autodl-tmp/taskA_cls   --out_dir /root/autodl-tmp/taskB_joint
```

---

### âš¡ Stage C â€“ Visual LoRA Joint Fine-Tuning  
`Qwen_pretrain/script/afterloradandc.py`

- Load LoRA adapters into the visual tower and continue joint training of classification + heatmap heads.  
- LoRA layers (default `[15, 23, 31]`) are injected via `peft`.  
- Similar arguments as Stage B, with additional losses (`--evi_alpha`, `--sparse_w`, `--contrast_w`).  
- Output (default `outputs_lora_joint/`) contains `best_by_AUROC_joint.pt`, `best_by_IoU_joint.pt`, checkpoints and logs.

**Example:**
```bash
python Qwen_pretrain/script/afterloradandc.py   --data_root /root/autodl-tmp/data   --train_ann /root/autodl-tmp/data/trainingset2/train_idx.json   --val_ann /root/autodl-tmp/data/trainingset2/val_idx.json   --model_path /root/autodl-tmp/Qwen2.5-VL-7B-Instruct   --clsA_dir /root/autodl-tmp/taskB_joint   --out_dir /root/autodl-tmp/taskC_lora
```

After training, organize the best weights as follows:
- `Qwen_pretrain/lora_adapter/` â€“ LoRA adapter (`adapter_model.safetensors`, `adapter_config.json`)  
- `Qwen_pretrain/head/best_by_AUROC_joint_lora.pt` â€“ forensic head (classification + heatmap)

---

## ğŸ§ª Task 7 â€“ Evaluation with Visual LoRA + Frozen Head

- **Script:** `Qwen_code/task7_vit_lora_eval/vit_head_eval.py`  
- **Config:** `Qwen_code/task7_vit_lora_eval/config_eval.json`  
- **Purpose:** Load the base model, merge LoRA, attach the frozen forensic head, and compute classification + heatmap metrics.

**Example:**
```bash
python Qwen_code/task7_vit_lora_eval/vit_head_eval.py   --config Qwen_code/task7_vit_lora_eval/config_eval.json
```

**Key Config Fields:**
- `base_model_path` â€“ path to Qwen2.5-VL-7B-Instruct  
- `visual_lora.path` & `visual_lora.layers` â€“ LoRA weights and injection layers  
- `head_checkpoint` â€“ Stage C forensic-head weights  
- `ann_eval`, `data_root` â€“ evaluation data  
- `metrics_csv`, `inference_output` â€“ paths to save metrics and predictions

---

## ğŸ’¬ Task 8 â€“ Multimodal Forensic Inference

<p align="center">
  <img src="modelimages/image1.png" alt="image1" width="80%" />
</p>

<p align="center">
  <img src="modelimages/image2.png" alt="image2" width="60%" />
</p>

- **Script:** `Qwen_code/task8_inference/multimodal_inference.py`  
- **Config:** `Qwen_code/task8_inference/config_inference.json`  
- **Features:**  
  - Load Task 7 visual and head weights  
  - Generate real/fake decisions, heatmap overlays, and explanatory text  
  - Support random sampling of prompt templates; save CSV and visual outputs

**Example:**
```bash
python Qwen_code/task8_inference/multimodal_inference.py   --config Qwen_code/task8_inference/config_inference.json
```

**Key Configs:**
- `system_prompt`, `chat_system_prompt`, `user_prompt` â€“ control dialogue templates  
- `generation` â€“ decoding params (`max_new_tokens`, `temperature`, `top_p`)  
- `output.csv`, `output.evidence_dir` â€“ paths for text and visual outputs  

---

## ğŸ§° Troubleshooting

- **Path not found** â†’ Scripts assume `/root/autodl-tmp` or `/root/data` prefix. Adjust via arguments or symlinks.  
- **OOM (Out of Memory)** â†’ Reduce `--batch_size`, or tune `batch_size` / `num_workers` in configs.  
- **LoRA not injected** â†’ Task 7/8 logs print injected modules. If missing, check `visual_lora.path`.  
- **Empty heatmap** â†’ Check `classification_threshold` / `heatmap_threshold` and mask annotations.  

---

With this pipeline, you can achieve an **end-to-end workflow** from visual LoRA training to **explainable multimodal inference**.  
You are encouraged to customize and extend the configurations or integrate new tasks on top of this framework.
